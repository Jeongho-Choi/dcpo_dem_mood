---
format:
  pdf:
    number-sections: true
    papersize: a4
crossref:
  sec-prefix: OSM
  sec-labels: alpha A
    
author:
  - name: Yuehong 'Cassandra' Tai
    affiliations:
      - ref: ui
    orcid: 0000-0001-7303-7443
    email: yuehong-tai@uiowa.edu
    equal-contributor: true
  - name: Yue Hu
    affiliations:
      - ref: tsu
    orcid: 0000-0002-2829-3971
    email: yuehu@tsinghua.edu.cn
    url: https://sammo3182.github.io
    equal-contributor: true
    corresponding: true
  - name: Frederick Solt
    affiliations:
      - ref: ui
    orcid: 0000-0002-3154-6132
    email: frederick-solt@uiowa.edu
    url: https://fsolt.org/
    equal-contributor: true

affiliations:
  - id: ui
    name: Department of Political Science, University of Iowa, Iowa, U.S.A.
  - id: tsu
    name: Department of Political Science, Tsinghua University, Beijing, China

filters:
    - authors-block
  
tables: true # enable longtable and booktabs
fontsize: 12pt
indent: true
geometry: margin=1in
linestretch: 1.5 # double spacing using linestretch 1.5
colorlinks: true
bibliography: p_dcpo_demsupport_corrigenda.bib
citation_package: natbib
link-citations: true
execute:
  echo: false
  message: false
  warning: false
  dpi: 300
    
title: "Corrigendum"
subtitle: |
  | Democracy, Public Support, and Measurement Uncertainty--CORRIGENDUM
  | doi:10.1017/S0003055422000429. Published in *American Political Science Review*: First View.
  
editor_options: 
  chunk_output_type: console
---

\newline
\newline

The authors regret that three operations raise controversies, and thus make the following updates:

# Nonrepresentative Surveys
**The Issue**: In the 2002 and 2005 Pew Global Attitudes Surveys, the codebooks identify a total of nineteen national surveys with samples that are biased toward urban areas.
Although Claassen [-@Claassen2020a; -@Claassen2020] identified and excluded seventeen of the nineteen (inadvertently, apparently, including two), we did not catch this issue at all and included all of these national surveys in our expanded sample.

**Resolution**: Data from these nineteen national surveys---including the two that were included in the sample used in Claassen [-@Claassen2020a; -@Claassen2020]---are now excluded from our data.


# Non-Responses
**The Issue**: In Claassen [-@Claassen2020a; -@Claassen2020], all respondents who did not respond to a survey item are assumed to not support democracy and an undemocratic response is (usually but not always, as we noted in the OSM at page A2; see also @HuEtAl2022a at 4) singly imputed to them.
Single imputation has long been well understood to incorporate overly strong assumptions and understate measurement error [see, e.g., @Rubin1987].
In the original paper, we treated these non-responses as 'missing at random'---that is, that the distribution of attitudes among those who did not respond was similar to that among those who did---and applied listwise deletion, excluding the non-responses from the sample.
This was a mistake: like single imputation, listwise deletion is known to understate measurement error [again see @Rubin1987 and many others]. 
So neither approach is suitable, especially not in a research on the importance of avoiding understating measurement error.

**Resolution**: In some cases (for example, the Americas Barometer survey of Canada in 2010; see @HuEtAl2022a at 5), non-responses are the result of split samples, where many respondents were not even presented with the survey item in question.
In such cases, imputing undemocratic responses to these respondents is clearly inapt.
Instead, they are missing at random---respondents were explicitly randomly assigned to the split samples that excluded the relevant item---and so listwise deletion is appropriate.
We therefore continue to exclude from the sample those respondents who were not even asked the relevant question.
These cases are rare, though; most non-responses in these surveys are the result of refusal, 'don't know', and other non-responsive answers.

For these remaining cases, it is _multiple_, not single, imputations that is the preferred method for addressing such missing data and the measurement error that results [see, e.g., @Rubin1987].
Typically, multiple imputation of survey non-response involves building a model of the missing data with the other variables of interest from the survey.
This preserves the relationships among these survey variables and allows the measurement uncertainty that remains after modeling to propagate to the quantity of interest [see, e.g., @KingEtAl2001; @BlackwellEtAl2017b].
Here, however, there is only one variable of interest (or, sometimes, a few) in each of thousands of national surveys, and building such an MI model for every survey is not practicable.

The single-imputation approach employed in Claassen [-@Claassen2020a; -@Claassen2020] is inappropriate on its own, but it points the way to a feasible solution that, unlike either single imputation or listwise deletion, incorporates measurement uncertainty in the proportion of the population that supports democracy.
To assume that all respondents who did not answer an item do not support democracy is not theoretically grounded: indeed, it is often argued that non-response to such politically sensitive items in nondemocracies reflects a fear of government reprisal for holding _pro-democratic_ views [see, e.g., @ShenTruex2021].
Nevertheless, the Claassen assumption sets a lower bound on the proportion of the population who support democracy as evidenced by a particular survey item.
Conversely, assuming that all non-responses reflect unvoiced _support_ for democracy establishes an upper bound on democratic support indicated by an item.
A more theoretically-driven assumption than either of these is that non-response in democratic contexts reflects social desirability bias and so a lack of support for democracy, while non-response in non-democratic contexts reflects fear of reprisal (or perhaps again social desirability bias, only yielding different results in a different political setting) and so democratic support.
An intermediate possibility, of course, is that attitudes among those who did not respond to an item are actually similar to the attitudes of those who did; that is, that the data are missing at random and non-responses are appropriately listwise deleted.

We therefore now incorporate the measurement uncertainty due to non-response as follows.
The number of democracy-supporting responses and the total sample size was imputed four times---that is, using in turn each of these four assumptions about the distribution of non-responses described above---for each country-year-item in the source survey data.
The latent variable was then estimated using each of these imputations, and the resulting draws combined as in model-based multiple imputation.
The result is a distribution of draws that reflects the measurement uncertainty in the latent variable, avoiding the strong assumptions of either single imputation or listwise deletion.


# Survey Weights
**The Issue**:  Claassen [-@Claassen2020a; -@Claassen2020] ignored survey weights (well, again usually but not always; see @HuEtAl2022a at 4--5), instead using the unweighted survey data.
This is problematic, of course, as the unweighted data may not be representative of the underlying population.
In our data collection, we corrected this by employing each survey's weights, but did not appreciate that some surveys' weights are not standardized; that is, applying them caused (occasionally dramatic) changes in the sample size.
As the sample size is an important input in the latent variable model---smaller samples yield larger uncertainty in the population mean, which then propagates into the estimate of democratic support---this is undesirable.

**Resolution**: We now standardize survey weights to have a mean of one before applying them and so ensure that these weights preserve the sample size.


# Results after the operation updates

Updating the replication materials to reflect these corrections (and, as a bonus, make them more user friendly); revising the tables in OSM B and the figures and tables in OSM D; adding a discussion of non-responses to OSM A; and updating Figure 1, Figure 2, and the specific numbers reported under "Adding More Data" in the text itself would be warranted.
Applying these corrections *does not* yield substantive differences in the results of our analyses and so does not affect the conclusions reached.

# References