---
output: 
  pdf_document:
    citation_package: natbib
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: svm-latex-ms2.tex
title: "[dcpo_demsupport]"
thanks: "Corresponding author: [yuehong-tai@uiowa.edu](mailto:yuehong-tai@uiowa.edu). Current version: `r format(Sys.time(), '%B %d, %Y')`."
author:
- name: Yuehong 'Cassandra' Tai
  affiliation: University of Iowa
- name: Yue Hu
  affiliation: Tsinghua University
- name: Frederick Solt
  affiliation: University of Iowa
abstract: ""
keywords: ""
date: "`r format(Sys.time(), '%B %d, %Y')`"
fontsize: 11pt
spacing: double
bibliography: \dummy{`r file.path(getwd(), list.files(getwd(), ".bib$", recursive = TRUE))`}
# csl: https://raw.githubusercontent.com/citation-style-language/styles/master/american-political-science-association.csl
biblio-style: apsr
citecolor: black
linkcolor: black
endnote: no
header-includes:
      - \usepackage{array}
      - \usepackage{caption}
      - \usepackage{graphicx}
      - \usepackage{siunitx}
      - \usepackage{colortbl}
      - \usepackage{multirow}
      - \usepackage{hhline}
      - \usepackage{calc}
      - \usepackage{tabularx}
      - \usepackage{threeparttable}
      - \usepackage{wrapfig}
nocite: |
  @Solt2020a
---

```{r setup, include=FALSE}
options(tinytex.verbose = TRUE)

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, dpi = 300)

# If you haven't install `DCPOtools`
# remotes::install_github("fsolt/DCPOtools")

if (!require(pacman)) install.packages("pacman")
library(pacman)

# load all the packages you will use below 
p_load(
  dataverse, # data scraping
  DCPOtools, # analysis
  huxtable, kableExtra, modelsummary, # tabulation
  latex2exp, # visualization
  rstan, # Bayesian estimation
  tidyverse # data wrangling
) 

# Functions preload
set.seed(313)
```

<!-- The following text is all cribbed from `fsolt/dcpo_article` and obviously needs heavy revision -->

# Incorporating Uncertainty
This means not simply employing the point estimates, that is, the means of the posterior distributions [_contra_ @Claassen2020a; @Claassen2020b].

Instead, researchers should follow the recommendations of work using other latent variables [e.g., @Schnakenberg2014; @Crabtree2015] and other measures incorporating uncertainty, such as the Standardized World Income Inequality Database [@Solt2020]: generate many duplicate versions of the analysis dataset, assign to each a different random draw from the posterior distributions of the variables measured with uncertainty, perform the analyses repeatedly on each of these multiple versions of the dataset, and combine the results following the rules set out in @Rubin1987.
The functional programming tools in the `purrr` package [@Henry2019] make this a matter of just a few additional lines of code.

```{r claassen_input_raw}
claassen_input_raw <- DCPOtools:::claassen_setup(vars = read_csv("data-raw/mood_dem.csv"),
                                       file = "data/claassen_input_raw.csv")
```

```{r claassen_input}
claassen_input_raw1 <- read_csv("data/claassen_input_raw.csv") %>% 
  filter(!(str_detect(item, "army") & # WVS obs identified as problematic by Claassen 
    ((country=="Albania" & year==1998) |
         (country=="Indonesia" & (year==2001 | year==2006)) |
         (country=="Iran" & year==2000) |
         (country=="Pakistan" & (year==1996 | year==2001)) |
         (country=="Vietnam" & year==2001)) |
    (str_detect(item, "strong") &
         ((country=="Egypt" & year==2012) |
              (country=="Iran" & (year==2000 | year==2005)) |
              (country=="India") |
              (country=="Pakistan" & (year==1996 | year==2001)) |
              (country=="Kyrgyzstan" & (year==2003 | year==2011)) |
              (country=="Romania" & (year==1998 | year==2005 | year==2012)) |
              (country=="Vietnam" & year==2001))))) %>% 
  filter(!(survey=="evs2017" | survey == "arabb5" | survey == "eb47")) %>%  # surveys unused by Claassen
  mutate(item = if_else(item == "strong_amb_1" & year == 2004, "strong_amb_2", item)) # items conflated in amb_combo file
  
claassen_input <- DCPOtools::format_claassen(claassen_input_raw)
```

```{r claassen_m5, eval=FALSE}
claassen_m5 <- rstan::stan(file = 'R/argon/dcpo_demsupport/R/supdem.stan.mod5.stan',
                           data = claassen_input,
                           iter = 2000,
                           warmup = 1000,
                           chains = 4,
                           cores = 4,
                           thin = 2,
                           pars = c("mu_lambda", "sigma_lambda", "sigma_delta", "sigma_theta", "phi", "lambda", "delta", "theta", "x_pred","log_lik"),
                           control = list(adapt_delta=0.99, stepsize=0.02, max_treedepth=15))

save(claassen_m5, file = "data/claassen_m5.RData")
```


```{r replication-model5, eval = FALSE}
# Before running the following lines, one should set their personal token and server in their system environment first: 
# Sys.setenv("DATAVERSE_KEY" = "exampleToken")
# Sys.setenv("DATAVERSE_SERVER" = "dataverse.harvard.edu")

supdem <- get_file("supdem raw survey marginals.tab", "doi:10.7910/DVN/HWLW0J") %>% 
  read_csv(col_types = cols(.default = col_double(), 
                            Country = col_character(),
                            CAbb = col_character(),
                            Project = col_character()))

# Required by the replication file
p_load(
  countrycode,
  MASS,
  psych,
  arm,
  loo,
  rstanarm,
  RColorBrewer,
  devtools,
  dgo
)

sddem1 = gather(data=supdem, key="Item", value="Response", 6:33, na.rm=TRUE) 
sddem1$Sample = 0
sddem1[sddem1$Project=="AfB", "Sample"] = sddem1[sddem1$Project=="AfB", "AfB_sample"]
sddem1[sddem1$Project=="ArB", "Sample"] = sddem1[sddem1$Project=="ArB", "ArB_sample"]
sddem1[sddem1$Project=="AsB", "Sample"] = sddem1[sddem1$Project=="AsB", "AsB_sample"]
sddem1[sddem1$Project=="AsnB", "Sample"] = sddem1[sddem1$Project=="AsnB", "AsnB_sample"]
sddem1[sddem1$Project=="ESS", "Sample"] = sddem1[sddem1$Project=="ESS", "ESS_sample"]
sddem1[sddem1$Project=="LAPOP", "Sample"] = sddem1[sddem1$Project=="LAPOP", "LAPOP_sample"]
sddem1[sddem1$Project=="LB", "Sample"] = sddem1[sddem1$Project=="LB", "LB_sample"]
sddem1[sddem1$Project=="NDB", "Sample"] = sddem1[sddem1$Project=="NDB", "NDB_sample"]
sddem1[sddem1$Project=="Pew", "Sample"] = sddem1[sddem1$Project=="Pew", "Pew_sample"]
sddem1[sddem1$Project=="WVS", "Sample"] = sddem1[sddem1$Project=="WVS", "WVS_sample"]
sddem1[sddem1$Project=="CSES", "Sample"] = sddem1[sddem1$Project=="CSES", "CSES_sample"]

# trim stacked dataset
sddem1 = sddem1[, -c(6:16)]
sddem1$Response = ifelse(sddem1$Response==0, NA, sddem1$Response)
sddem1 = sddem1[!is.na(sddem1$Response), ] 

# create item by project indicators
sddem1 = unite(sddem1, ItemProj, c(Project, Item), sep = "_", remove = FALSE)
sddem1$ItemProj = as.factor(sddem1$ItemProj)

# create year by country indicators
year0 = 1990
sddem1 = sddem1[sddem1$Year > year0,]
sddem1 = unite(sddem1, YearCountry, c(Year, Country), sep = "_", remove = FALSE)
sddem1$YearCountry = as.factor(sddem1$YearCountry)

# create item by project by country indicators
sddem1 = unite(sddem1, ItemProjCnt, c(ItemProj, Country), sep = "_", remove = FALSE)
sddem1$ItemProjCnt = as.factor(sddem1$ItemProjCnt)

# create item by country indicators
sddem1 = unite(sddem1, ItemCnt, c(Item, Country), sep = "_", remove = FALSE)
sddem1$ItemCnt = as.factor(sddem1$ItemCnt)

# create year by project indicators
sddem1 = unite(sddem1, YrProj, c(Year, Project), sep = "_", remove = FALSE)
sddem1$YrProj = as.factor(sddem1$YrProj)

# create year by project by country indicators
sddem1 = unite(sddem1, YrProjCnt, c(YrProj, Country), sep = "_", remove = FALSE)
sddem1$YrProjCnt = as.factor(sddem1$YrProjCnt)

# factorise
sddem1$Country = as.factor(as.character(sddem1$Country))
sddem1$Item = as.factor(as.character(sddem1$Item))
sddem1$ItemProj = as.factor(as.character(sddem1$ItemProj))
sddem1$ItemCnt = as.factor(as.character(sddem1$ItemCnt))
sddem1$ItemProjCnt = as.factor(as.character(sddem1$ItemProjCnt))
sddem1$Project = as.factor(as.character(sddem1$Project))
sddem1$YrProj = as.factor(as.character(sddem1$YrProj))
sddem1$YrProjCnt = as.factor(as.character(sddem1$YrProjCnt))
sddem1$Year = as.factor(sddem1$Year-year0)

# drop countries with less than 2 years of data
cnt.obs.years = rowSums(table(sddem1$Country, sddem1$Year) > 0)
sddem2 = sddem1[sddem1$Country %in% levels(sddem1$Country)[cnt.obs.years > 1], ]

## Stan estimation

# prepare data for stan

n.items = length(unique(sddem2$Item))
n.cntrys = length(unique(sddem2$Country))
n.yrs = 2015-year0
n.proj = length(unique(sddem2$Project))
n.resp = dim(sddem2)[1]
n.itm.prj = length(unique(sddem2$ItemProj))
n.itm.cnt = length(unique(sddem2$ItemCnt))
n.cntry.yrs = n.cntrys * n.yrs
n.itm.prj.cnt = length(unique(sddem2$ItemProjCnt))
n.yr.proj.cnt = length(unique(sddem2$YrProjCnt))
cntrys = as.numeric(factor(sddem2$Country))
cnt.names = sort(unique(sddem2$Country))
cnt.ccode = sddem2[match(cnt.names, sddem2$Country), "COWCode"]
items = as.numeric(factor(sddem2$Item))
yrs = as.numeric(sddem2$Year)
projs = as.numeric(factor(sddem2$Project))
itm.prjs = as.numeric(factor(sddem2$ItemProj))
itm.prj.cnts = as.numeric(factor(sddem2$ItemProjCnt))
itm.cnts = as.numeric(factor(sddem2$ItemCnt))
cntry.yrs = as.numeric(sddem2$YearCountry)

# specify data for stan
dat.2 = list(N=n.resp, K=n.itm.prj, T=n.yrs, J=n.cntrys, P=n.itm.prj.cnt, jj=cntrys, tt=yrs, 
             pp=itm.prj.cnts, kk=itm.prjs, x=sddem2$Response, samp=sddem2$Sample)

# pars
pars.5 = c("mu_lambda","sigma_lambda","sigma_delta","sigma_theta","phi","lambda","delta","theta","x_pred","log_lik")

# set pars for final run (time-consuming; reduce iterations for trial run)
n.iter = 1000
n.warm = 500
n.chn = 4
n.thin = 2

## supdem.stan.mod5.stan obtained from dataverse replication files
stan.mod.5 <- stan(file = "supdem.stan.mod5.stan", data=dat.2, pars=pars.5, 
                   iter=n.iter, warmup=n.warm, chains=n.chn, thin=n.thin, 
                   control=list(adapt_delta=0.99, stepsize=0.02, max_treedepth=11))
```


# A Better Measure of Democratic Support
The DCPO model is estimated using the `DCPO` package for R [@Solt2020a], which is written in the Stan probabilistic programming language [@StanDevTeam2019a; @StanDevTeam2019b].

## Relative Fit
<!-- We'll only compare Model 5 and DCPO and save ourselves the heartache of dealing with dgirt, but the comparison needs updated with the full dataset -->
To assess the ability of the DCPO model to fit public opinion data relative to the two alternative approaches, I use the set of survey questions on support for democracy employed in @Claassen2019 [, 7-8].

The first three columns of Table \ref{tab:validation_table} present the results of an internal validation test, that is, a test that uses the same data that was used to fit the model [see, e.g., @Claassen2019, 9].
In column 1, the mean absolute error (MAE) measures the average difference between the observed proportion of survey respondents in country $k$ in year $t$ with replies to question $q$ with a response at least as positive as response $r$ (or with an affirmative response, in the case of Claassen's [-@Claassen2019] Model 5) and the model's predicted proportion across all countries, years, questions, and response categories.
Given, however, that Claassen's [-@Claassen2019] Model 5 is fit to dichotomized data, while the @Caughey2019 and DCPO models are fit to the original, possibly ordinal, survey data, which have higher variance, comparing the MAE across all three models can be somewhat misleading.
Therefore, the MAE of the country means---the average proportions answering affirmatively (for Claassen's [-@Claassen2019] Model 5) or with a response at least as positive as response $r$ (for @Caughey2019 and DCPO) in each country across all years and all questions---serve as a baseline [see @Claassen2019, 11-12]; these appear in column 2.
The percentage reduction in MAE achieved by each model, listed in column 3, represents the improvement in fit compared to this baseline.

The last three columns of Table \ref{tab:validation_table} present the results of an external validation test, a test of the models' ability to predict out-of-sample survey responses.
This test employs $k$-fold validation with 10 folds, that is, it randomly divides the available country-year-questions into tenths and then sequentially treats each tenth as a test set to be predicted while fitting the model on a training set consisting of the other nine tenths of the data.
Column 4 shows the mean of the MAEs of the ten resulting sets of out-of-sample predictions, while column 5 presents the mean improvement over of these ten MAEs over their respective ten country-mean MAEs.
Column 6 reports the discrepancy between the percentage of all response proportions that fall within their corresponding predictions' 80% confidence intervals and the expected 80%; it therefore provides a gauge of the accuracy of the models' estimates of uncertainty.
A negative value in this column indicate that less than 80% of the observed out-of-sample survey proportions are included with the model's predictions' nominal 80% credible intervals, and so its uncertainty estimates are overconfident, while a positive value indicates the opposite, that more than 80% of the observed proportions fall within the credible intervals and so the model's uncertainty estimates are overly conservative.

```{r validation}
# load(here::here("data", "validation_table.rda"))
# 
# validation_table
```

Comparing first the results for Claassen's [-@Claassen2019] Model 5 with those for the @Caughey2019 model reveals that, at least for this set of survey questions, while the former has a smaller MAE, the latter accounts for a larger percentage of the variation in its source data left unexplained by its respective country-means model.
This is true both in the internal validation test and on average across the ten folds of the external validation test.
Again, this discrepancy is possible due to the greater variance in the survey data when its ordinal nature is preserved, as in the @Caughey2019 model, rather than dichotomized, as in Claassen's [-@Claassen2019] approach.
An examination of column 6 reveals that the @Caughey2019 model yielded predictions with credible intervals that are much too narrow, encompassing only about 13% of the actual sample observations, while those for Claassen's [-@Claassen2019] Model 5 were slightly too conservative.^[
The relatively poor uncertainty estimates of the @Caughey2019 model are similar to those found for the dichotomous version presented in @Caughey2015 by @Claassen2019 [, 12].]

However, it is the DCPO model that provides what is unambiguously the best fit.
It features the smallest mean absolute error: on average, in the internal validation test its predictions are just over 3 percentage points away from the actual sample observations, and in the external validation test they are only 5.5 percentage points away from the actual out-of-sample observations.
In both cases, these represent the largest percentage improvement over the country-means MAE.
Further, in the external validation test, the DCPO credible intervals come closest to matching the nominal 80% level of any of the three models.

## Results
DCPO results here


# Conclusion

\pagebreak

